{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Child Mind Institute - BFRB Detection Competition\n",
    "## Final Submission by Shail Shah\n",
    "\n",
    "This notebook provides a memory-optimized solution for the Kaggle competition on detecting Body-Focused Repetitive Behaviors (BFRBs) from sensor data. The implementation follows the competition evaluation metric and submission requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# Signal processing and scientific computing\n",
    "from scipy import stats, signal\n",
    "from scipy.fftpack import fft, rfft\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Machine learning libraries\n",
    "## scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GroupKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Deep learning libraries (import conditionally to avoid errors if not installed)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available. Using classical ML models only.\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not available.\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "    print(\"LightGBM not available.\")\n",
    "\n",
    "# Serialization\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Set plotting style\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "if TORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "# Disable warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Path to the competition data\n",
    "ZIP_PATH = '/path/to/cmi-detect-behavior-with-sensor-data.zip'\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(ZIP_PATH):\n",
    "    print(f\"Zip file found: {os.path.getsize(ZIP_PATH) / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(\"Zip file not found. Please update the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory-Efficient Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SensorDataProcessor:\n",
    "    \"\"\"Memory-efficient data processor for BFRB detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, zip_path, cache_dir=None):\n",
    "        \"\"\"Initialize with path to zip file.\"\"\"\n",
    "        self.zip_path = zip_path\n",
    "        self.cache_dir = cache_dir\n",
    "        self.binary_encoder = None\n",
    "        self.gesture_encoder = None\n",
    "    \n",
    "    def load_demographics(self):\n",
    "        \"\"\"Load demographics data from the zip file.\"\"\"\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "            with zip_ref.open('train_demographics.csv') as f:\n",
    "                train_demo = pd.read_csv(f)\n",
    "            with zip_ref.open('test_demographics.csv') as f:\n",
    "                test_demo = pd.read_csv(f)\n",
    "        return train_demo, test_demo\n",
    "    \n",
    "    def get_sequence_ids(self, file_path, max_chunks=None):\n",
    "        \"\"\"Extract unique sequence IDs from the CSV file.\"\"\"\n",
    "        sequence_ids = []\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "            with zip_ref.open(file_path) as f:\n",
    "                chunks = pd.read_csv(f, chunksize=100000)\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    if max_chunks is not None and i >= max_chunks:\n",
    "                        break\n",
    "                    sequence_ids.extend(chunk['sequence_id'].unique())\n",
    "                    \n",
    "        # Remove duplicates and sort\n",
    "        sequence_ids = sorted(list(set(sequence_ids)))\n",
    "        return sequence_ids\n",
    "    \n",
    "    def extract_sequence(self, file_path, sequence_id):\n",
    "        \"\"\"Extract a specific sequence from the CSV file.\"\"\"\n",
    "        sequence_df = None\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "            with zip_ref.open(file_path) as f:\n",
    "                chunks = pd.read_csv(f, chunksize=100000)\n",
    "                for chunk in chunks:\n",
    "                    seq_data = chunk[chunk['sequence_id'] == sequence_id]\n",
    "                    if len(seq_data) > 0:\n",
    "                        if sequence_df is None:\n",
    "                            sequence_df = seq_data\n",
    "                        else:\n",
    "                            sequence_df = pd.concat([sequence_df, seq_data])\n",
    "        \n",
    "        # Sort by sequence counter\n",
    "        if sequence_df is not None:\n",
    "            sequence_df = sequence_df.sort_values('sequence_counter')\n",
    "        \n",
    "        return sequence_df\n",
    "    \n",
    "    def preprocess_sequence(self, sequence_df):\n",
    "        \"\"\"Apply preprocessing to a sequence DataFrame.\"\"\"\n",
    "        # Handle missing values\n",
    "        # For time-of-flight data, -1 indicates no reading\n",
    "        # For other sensors, NaN indicates missing data\n",
    "        \n",
    "        # Fill NaN values with forward fill then backward fill\n",
    "        sequence_df = sequence_df.fillna(method='ffill').fillna(method='bfill')\n",
    "        return sequence_df\n",
    "    \n",
    "    def extract_statistical_features(self, sequence_df):\n",
    "        \"\"\"Extract statistical features from a sequence.\"\"\"\n",
    "        # Get basic information\n",
    "        sequence_id = sequence_df['sequence_id'].iloc[0]\n",
    "        subject = sequence_df['subject'].iloc[0]\n",
    "        \n",
    "        # Create feature dict\n",
    "        features = {\n",
    "            'sequence_id': sequence_id,\n",
    "            'subject': subject\n",
    "        }\n",
    "        \n",
    "        # Add sequence type and gesture if available\n",
    "        if 'sequence_type' in sequence_df.columns:\n",
    "            features['sequence_type'] = sequence_df['sequence_type'].iloc[0]\n",
    "        if 'gesture' in sequence_df.columns:\n",
    "            features['gesture'] = sequence_df['gesture'].iloc[0]\n",
    "        \n",
    "        # Get sensor columns\n",
    "        acc_cols = ['acc_x', 'acc_y', 'acc_z']\n",
    "        rot_cols = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "        thm_cols = [f'thm_{i}' for i in range(1, 6)]\n",
    "        \n",
    "        # Calculate features for each sensor type\n",
    "        for col_prefix, cols in [\n",
    "            ('acc', acc_cols), \n",
    "            ('rot', rot_cols), \n",
    "            ('thm', thm_cols)\n",
    "        ]:\n",
    "            for col in cols:\n",
    "                values = sequence_df[col].values\n",
    "                \n",
    "                # Basic statistics\n",
    "                features[f'{col}_mean'] = np.mean(values)\n",
    "                features[f'{col}_std'] = np.std(values)\n",
    "                features[f'{col}_min'] = np.min(values)\n",
    "                features[f'{col}_max'] = np.max(values)\n",
    "                features[f'{col}_median'] = np.median(values)\n",
    "                features[f'{col}_range'] = np.max(values) - np.min(values)\n",
    "                \n",
    "                # Higher-order statistics\n",
    "                features[f'{col}_skew'] = skew(values)\n",
    "                features[f'{col}_kurtosis'] = kurtosis(values)\n",
    "                \n",
    "                # Percentiles\n",
    "                features[f'{col}_q25'] = np.percentile(values, 25)\n",
    "                features[f'{col}_q75'] = np.percentile(values, 75)\n",
    "                features[f'{col}_iqr'] = features[f'{col}_q75'] - features[f'{col}_q25']\n",
    "                \n",
    "                # Signal properties\n",
    "                if len(values) >= 2:\n",
    "                    # Absolute differences\n",
    "                    diffs = np.diff(values)\n",
    "                    features[f'{col}_diff_mean'] = np.mean(np.abs(diffs))\n",
    "                    features[f'{col}_diff_std'] = np.std(diffs)\n",
    "                    features[f'{col}_diff_max'] = np.max(np.abs(diffs))\n",
    "                    \n",
    "                    # FFT features (first few coefficients)\n",
    "                    if len(values) >= 5:\n",
    "                        fft_vals = np.abs(fft(values - np.mean(values)))\n",
    "                        for i in range(3):\n",
    "                            if i < len(fft_vals) // 2:\n",
    "                                features[f'{col}_fft_{i}'] = fft_vals[i]\n",
    "        \n",
    "        # Time-of-flight (ToF) sensor aggregation\n",
    "        for sensor in range(1, 6):\n",
    "            # Get all ToF columns for this sensor\n",
    "            tof_cols = [f'tof_{sensor}_v{i}' for i in range(64)]\n",
    "            tof_data = sequence_df[tof_cols].replace(-1, np.nan)\n",
    "            \n",
    "            # Aggregate across time steps and values\n",
    "            features[f'tof_{sensor}_mean'] = tof_data.mean().mean()\n",
    "            features[f'tof_{sensor}_std'] = tof_data.std().mean()\n",
    "            features[f'tof_{sensor}_missing'] = tof_data.isna().sum().sum() / (len(sequence_df) * 64)\n",
    "            features[f'tof_{sensor}_min'] = tof_data.min().min()\n",
    "            features[f'tof_{sensor}_max'] = tof_data.max().max()\n",
    "            \n",
    "            # Grid-based features\n",
    "            # Reshape as 8x8 grid and get statistics on different regions\n",
    "            frame_count = min(5, len(sequence_df))\n",
    "            for frame_idx in range(frame_count):\n",
    "                if frame_idx < len(sequence_df):\n",
    "                    frame = sequence_df.iloc[frame_idx]\n",
    "                    grid = np.zeros((8, 8))\n",
    "                    for i in range(64):\n",
    "                        grid[i // 8, i % 8] = frame[f'tof_{sensor}_v{i}']\n",
    "                    \n",
    "                    # Center vs periphery comparison\n",
    "                    center = grid[2:6, 2:6]\n",
    "                    periphery = np.concatenate([\n",
    "                        grid[0:2, :].flatten(),\n",
    "                        grid[6:8, :].flatten(),\n",
    "                        grid[2:6, 0:2].flatten(),\n",
    "                        grid[2:6, 6:8].flatten()\n",
    "                    ])\n",
    "                    \n",
    "                    # Replace -1 with NaN\n",
    "                    center = center[center != -1]\n",
    "                    periphery = periphery[periphery != -1]\n",
    "                    \n",
    "                    if len(center) > 0:\n",
    "                        features[f'tof_{sensor}_center_mean_f{frame_idx}'] = np.mean(center)\n",
    "                    if len(periphery) > 0:\n",
    "                        features[f'tof_{sensor}_periph_mean_f{frame_idx}'] = np.mean(periphery)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def process_sequences_batch(self, file_path, output_file=None, max_sequences=None):\n",
    "        \"\"\"Process all sequences and extract features.\"\"\"\n",
    "        # Get all sequence IDs\n",
    "        sequence_ids = self.get_sequence_ids(file_path)\n",
    "        if max_sequences is not None:\n",
    "            sequence_ids = sequence_ids[:max_sequences]\n",
    "        \n",
    "        # Process each sequence\n",
    "        all_features = []\n",
    "        for seq_id in tqdm(sequence_ids, desc=\"Processing sequences\"):\n",
    "            # Extract and preprocess sequence\n",
    "            sequence_df = self.extract_sequence(file_path, seq_id)\n",
    "            if sequence_df is None or len(sequence_df) == 0:\n",
    "                print(f\"Warning: Sequence {seq_id} is empty or not found.\")\n",
    "                continue\n",
    "                \n",
    "            sequence_df = self.preprocess_sequence(sequence_df)\n",
    "            \n",
    "            # Extract features\n",
    "            features = self.extract_statistical_features(sequence_df)\n",
    "            all_features.append(features)\n",
    "            \n",
    "            # Force garbage collection\n",
    "            del sequence_df\n",
    "            gc.collect()\n",
    "        \n",
    "        # Create DataFrame\n",
    "        features_df = pd.DataFrame(all_features)\n",
    "        \n",
    "        # Save to file if specified\n",
    "        if output_file is not None:\n",
    "            features_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def prepare_for_training(self, features_df, imu_only=False):\n",
    "        \"\"\"Prepare features for training.\"\"\"\n",
    "        # Create binary labels (Target/Non-Target)\n",
    "        binary_labels = features_df['sequence_type'].map({'Target': 1, 'Non-Target': 0}).values\n",
    "        \n",
    "        # Create multi-class labels (specific gestures)\n",
    "        if self.gesture_encoder is None:\n",
    "            self.gesture_encoder = LabelEncoder()\n",
    "            self.gesture_encoder.fit(features_df['gesture'])\n",
    "        \n",
    "        if self.binary_encoder is None:\n",
    "            self.binary_encoder = LabelEncoder()\n",
    "            self.binary_encoder.fit(['Non-Target', 'Target'])\n",
    "        \n",
    "        multi_labels = self.gesture_encoder.transform(features_df['gesture'])\n",
    "        \n",
    "        # Select features\n",
    "        if imu_only:\n",
    "            # Use only IMU features (accelerometer and rotation)\n",
    "            feature_cols = [col for col in features_df.columns \n",
    "                           if col.startswith('acc_') or col.startswith('rot_')]\n",
    "        else:\n",
    "            # Use all sensor features\n",
    "            feature_cols = [col for col in features_df.columns \n",
    "                           if any(col.startswith(prefix) for prefix in \n",
    "                                 ['acc_', 'rot_', 'thm_', 'tof_'])]\n",
    "        \n",
    "        # Extract features\n",
    "        X = features_df[feature_cols].values\n",
    "        \n",
    "        return X, binary_labels, multi_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    class SimplifiedModel(nn.Module):\n",
    "        \"\"\"Neural network model for BFRB detection using statistical features.\"\"\"\n",
    "        \n",
    "        def __init__(self, input_dim, hidden_dims=[256, 128, 64], num_gestures=10, dropout=0.3):\n",
    "            super(SimplifiedModel, self).__init__()\n",
    "            \n",
    "            # Input layer\n",
    "            layers = [nn.Linear(input_dim, hidden_dims[0]), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(len(hidden_dims) - 1):\n",
    "                layers.extend([\n",
    "                    nn.Linear(hidden_dims[i], hidden_dims[i+1]),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout)\n",
    "                ])\n",
    "            \n",
    "            # Common feature extractor\n",
    "            self.feature_extractor = nn.Sequential(*layers)\n",
    "            \n",
    "            # Binary classification head (Target vs. Non-Target)\n",
    "            self.binary_head = nn.Linear(hidden_dims[-1], 2)\n",
    "            \n",
    "            # Multi-class classification head (specific gestures)\n",
    "            self.multi_head = nn.Linear(hidden_dims[-1], num_gestures)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Extract features\n",
    "            features = self.feature_extractor(x)\n",
    "            \n",
    "            # Binary classification\n",
    "            binary_logits = self.binary_head(features)\n",
    "            \n",
    "            # Multi-class classification\n",
    "            multi_logits = self.multi_head(features)\n",
    "            \n",
    "            return binary_logits, multi_logits\n",
    "    \n",
    "    class SensorFeatureDataset(Dataset):\n",
    "        \"\"\"Dataset for statistical features extracted from sensor data.\"\"\"\n",
    "        \n",
    "        def __init__(self, features, binary_labels, multi_labels=None):\n",
    "            self.features = torch.tensor(features, dtype=torch.float32)\n",
    "            self.binary_labels = torch.tensor(binary_labels, dtype=torch.long)\n",
    "            \n",
    "            if multi_labels is not None:\n",
    "                self.multi_labels = torch.tensor(multi_labels, dtype=torch.long)\n",
    "            else:\n",
    "                self.multi_labels = None\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.features)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            if self.multi_labels is not None:\n",
    "                return self.features[idx], self.binary_labels[idx], self.multi_labels[idx]\n",
    "            else:\n",
    "                return self.features[idx], self.binary_labels[idx], -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Competition Metric Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def competition_score(binary_preds, binary_true, multi_preds, multi_true):\n",
    "    \"\"\"\n",
    "    Calculate the competition metric according to Kaggle's requirements:\n",
    "    1. Binary F1: Macro F1 score for target vs non-target classification\n",
    "    2. Gesture F1: Macro F1 score for gesture classification, where only target sequences are considered\n",
    "    Final score is the average of these two components.\n",
    "    \"\"\"\n",
    "    # 1. Binary F1 (target vs non-target)\n",
    "    binary_f1 = f1_score(binary_true, binary_preds, average='macro')\n",
    "    \n",
    "    # 2. Multi-class F1 (gesture classification for target sequences only)\n",
    "    # Only evaluate on target sequences (where binary_true == 1)\n",
    "    is_target = binary_true == 1\n",
    "    \n",
    "    # If there are target sequences in this batch, calculate F1\n",
    "    if np.sum(is_target) > 0:\n",
    "        multi_f1 = f1_score(\n",
    "            multi_true[is_target],  # True gesture labels for target sequences\n",
    "            multi_preds[is_target],  # Predicted gesture labels for target sequences\n",
    "            average='macro'\n",
    "        )\n",
    "    else:\n",
    "        # No target sequences in this batch\n",
    "        multi_f1 = 0.0\n",
    "    \n",
    "    # Final score is the average of the two components\n",
    "    final_score = (binary_f1 + multi_f1) / 2\n",
    "    \n",
    "    return final_score, binary_f1, multi_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_model(processor, features_df, output_dir='models', n_folds=5,\n",
    "               batch_size=32, epochs=30, imu_only=False, group_cv=True):\n",
    "    \"\"\"Train a model with cross-validation.\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare features for training\n",
    "    X, y_binary, y_multi = processor.prepare_for_training(features_df, imu_only=imu_only)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Save scaler and encoders\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.joblib'))\n",
    "    joblib.dump(processor.binary_encoder, os.path.join(output_dir, 'binary_encoder.joblib'))\n",
    "    joblib.dump(processor.gesture_encoder, os.path.join(output_dir, 'gesture_encoder.joblib'))\n",
    "    joblib.dump(processor, os.path.join(output_dir, 'processor.joblib'))\n",
    "    \n",
    "    # Train a fallback model in case neural network training fails\n",
    "    fallback_model = None\n",
    "    \n",
    "    # Try training a RandomForest as fallback\n",
    "    try:\n",
    "        print(\"Training RandomForest fallback model...\")\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X, y_binary)  # Train on binary classification task\n",
    "        fallback_model = rf\n",
    "        joblib.dump(fallback_model, os.path.join(output_dir, 'fallback_model.joblib'))\n",
    "        print(\"Fallback model saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to train fallback model: {e}\")\n",
    "    \n",
    "    if TORCH_AVAILABLE:\n",
    "        try:\n",
    "            print(\"Training neural network model...\")\n",
    "            # Set up cross-validation\n",
    "            if group_cv:\n",
    "                # Group by subject to prevent data leakage\n",
    "                groups = features_df['subject'].values\n",
    "                cv = GroupKFold(n_splits=n_folds)\n",
    "                splits = list(cv.split(X, y_binary, groups=groups))\n",
    "            else:\n",
    "                cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "                splits = list(cv.split(X, y_binary))\n",
    "            \n",
    "            # Set device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            print(f\"Using device: {device}\")\n",
    "            \n",
