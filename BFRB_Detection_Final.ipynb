{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Child Mind Institute - BFRB Detection Competition Submission\n",
    "## by Shail Shah\n",
    "\n",
    "This notebook contains a complete solution for the Child Mind Institute Body-Focused Repetitive Behaviors (BFRB) Detection competition. The solution includes:\n",
    "\n",
    "1. Data exploration and visualization\n",
    "2. Memory-efficient data processing\n",
    "3. Feature extraction\n",
    "4. Model implementation\n",
    "5. Training pipeline\n",
    "6. Kaggle-compliant inference API\n",
    "7. Submission file generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import joblib\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# PyTorch libraries\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available. Using alternative models.\")\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Path to the competition data\n",
    "ZIP_PATH = '/path/to/cmi-detect-behavior-with-sensor-data.zip'\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(ZIP_PATH):\n",
    "    print(f\"Zip file found: {os.path.getsize(ZIP_PATH) / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(\"Zip file not found. Please update the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SensorDataProcessor:\n",
    "    \"\"\"Memory-efficient data processor for BFRB detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, zip_path, cache_dir=None):\n",
    "        \"\"\"Initialize with path to zip file.\"\"\"\n",
    "        self.zip_path = zip_path\n",
    "        self.cache_dir = cache_dir\n",
    "        self.binary_encoder = None\n",
    "        self.gesture_encoder = None\n",
    "        \n",
    "    def load_demographics(self):\n",
    "        \"\"\"Load demographics data from the zip file.\"\"\"\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "            with zip_ref.open('train_demographics.csv') as f:\n",
    "                train_demo = pd.read_csv(f)\n",
    "            with zip_ref.open('test_demographics.csv') as f:\n",
    "                test_demo = pd.read_csv(f)\n",
    "        return train_demo, test_demo\n",
    "    \n",
    "    def get_sequence_ids(self, file_path, max_chunks=None):\n",
    "        \"\"\"Extract unique sequence IDs from the CSV file.\"\"\"\n",
    "        sequence_ids = []\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "            with zip_ref.open(file_path) as f:\n",
    "                chunks = pd.read_csv(f, chunksize=100000)\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    if max_chunks is not None and i >= max_chunks:\n",
    "                        break\n",
    "                    sequence_ids.extend(chunk['sequence_id'].unique())\n",
    "                    \n",
    "        # Remove duplicates and sort\n",
    "        sequence_ids = sorted(list(set(sequence_ids)))\n",
    "        return sequence_ids\n",
    "    \n",
    "    def extract_sequence(self, file_path, sequence_id):\n",
    "        \"\"\"Extract a specific sequence from the CSV file.\"\"\"\n",
    "        sequence_df = None\n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "            with zip_ref.open(file_path) as f:\n",
    "                chunks = pd.read_csv(f, chunksize=100000)\n",
    "                for chunk in chunks:\n",
    "                    seq_data = chunk[chunk['sequence_id'] == sequence_id]\n",
    "                    if len(seq_data) > 0:\n",
    "                        if sequence_df is None:\n",
    "                            sequence_df = seq_data\n",
    "                        else:\n",
    "                            sequence_df = pd.concat([sequence_df, seq_data])\n",
    "        \n",
    "        # Sort by sequence counter\n",
    "        if sequence_df is not None:\n",
    "            sequence_df = sequence_df.sort_values('sequence_counter')\n",
    "        \n",
    "        return sequence_df\n",
    "    \n",
    "    def preprocess_sequence(self, sequence_df):\n",
    "        \"\"\"Apply preprocessing to a sequence DataFrame.\"\"\"\n",
    "        # Handle missing values\n",
    "        # For time-of-flight data, -1 indicates no reading\n",
    "        # For other sensors, NaN indicates missing data\n",
    "        \n",
    "        # Fill NaN values with forward fill then backward fill\n",
    "        sequence_df = sequence_df.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        return sequence_df\n",
    "    \n",
    "    def extract_statistical_features(self, sequence_df):\n",
    "        \"\"\"Extract statistical features from a sequence.\"\"\"\n",
    "        # Get basic information\n",
    "        sequence_id = sequence_df['sequence_id'].iloc[0]\n",
    "        subject = sequence_df['subject'].iloc[0]\n",
    "        \n",
    "        # Create feature dict\n",
    "        features = {\n",
    "            'sequence_id': sequence_id,\n",
    "            'subject': subject\n",
    "        }\n",
    "        \n",
    "        # Get sensor columns\n",
    "        acc_cols = ['acc_x', 'acc_y', 'acc_z']\n",
    "        rot_cols = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "        thm_cols = [f'thm_{i}' for i in range(1, 6)]\n",
    "        \n",
    "        # Calculate features for each sensor type\n",
    "        for col_prefix, cols in [\n",
    "            ('acc', acc_cols), \n",
    "            ('rot', rot_cols), \n",
    "            ('thm', thm_cols)\n",
    "        ]:\n",
    "            for col in cols:\n",
    "                values = sequence_df[col].values\n",
    "                \n",
    "                # Basic statistics\n",
    "                features[f'{col}_mean'] = np.mean(values)\n",
    "                features[f'{col}_std'] = np.std(values)\n",
    "                features[f'{col}_min'] = np.min(values)\n",
    "                features[f'{col}_max'] = np.max(values)\n",
    "                features[f'{col}_median'] = np.median(values)\n",
    "                features[f'{col}_range'] = np.max(values) - np.min(values)\n",
    "                \n",
    "                # Percentiles\n",
    "                features[f'{col}_q25'] = np.percentile(values, 25)\n",
    "                features[f'{col}_q75'] = np.percentile(values, 75)\n",
    "                features[f'{col}_iqr'] = features[f'{col}_q75'] - features[f'{col}_q25']\n",
    "                \n",
    "                # Signal properties\n",
    "                if len(values) >= 2:\n",
    "                    # Absolute differences\n",
    "                    diffs = np.diff(values)\n",
    "                    features[f'{col}_diff_mean'] = np.mean(np.abs(diffs))\n",
    "                    features[f'{col}_diff_std'] = np.std(diffs)\n",
    "                    features[f'{col}_diff_max'] = np.max(np.abs(diffs))\n",
    "                \n",
    "        # Time-of-flight (ToF) sensor aggregation\n",
    "        for sensor in range(1, 6):\n",
    "            # Get all ToF columns for this sensor\n",
    "            tof_cols = [f'tof_{sensor}_v{i}' for i in range(64)]\n",
    "            tof_data = sequence_df[tof_cols].replace(-1, np.nan)\n",
    "            \n",
    "            # Aggregate across time steps and values\n",
    "            features[f'tof_{sensor}_mean'] = tof_data.mean().mean()\n",
    "            features[f'tof_{sensor}_std'] = tof_data.std().mean()\n",
    "            features[f'tof_{sensor}_missing'] = tof_data.isna().sum().sum() / (len(sequence_df) * 64)\n",
    "            features[f'tof_{sensor}_min'] = tof_data.min().min()\n",
    "            features[f'tof_{sensor}_max'] = tof_data.max().max()\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def process_sequences_batch(self, file_path, output_file=None, max_sequences=None):\n",
    "        \"\"\"Process all sequences and extract features.\"\"\"\n",
    "        # Get all sequence IDs\n",
    "        sequence_ids = self.get_sequence_ids(file_path)\n",
    "        if max_sequences is not None:\n",
    "            sequence_ids = sequence_ids[:max_sequences]\n",
    "        \n",
    "        # Process each sequence\n",
    "        all_features = []\n",
    "        for seq_id in tqdm(sequence_ids, desc=\"Processing sequences\"):\n",
    "            # Extract and preprocess sequence\n",
    "            sequence_df = self.extract_sequence(file_path, seq_id)\n",
    "            if sequence_df is None or len(sequence_df) == 0:\n",
    "                print(f\"Warning: Sequence {seq_id} is empty or not found.\")\n",
    "                continue\n",
    "                \n",
    "            sequence_df = self.preprocess_sequence(sequence_df)\n",
    "            \n",
    "            # Extract features\n",
    "            features = self.extract_statistical_features(sequence_df)\n",
    "            all_features.append(features)\n",
    "            \n",
    "            # Force garbage collection\n",
    "            del sequence_df\n",
    "            gc.collect()\n",
    "        \n",
    "        # Create DataFrame\n",
    "        features_df = pd.DataFrame(all_features)\n",
    "        \n",
    "        # Save to file if specified\n",
    "        if output_file is not None:\n",
    "            features_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def prepare_for_training(self, features_df, imu_only=False):\n",
    "        \"\"\"Prepare features for training.\"\"\"\n",
    "        # Create binary labels (Target/Non-Target)\n",
    "        binary_labels = features_df['sequence_type'].map({'Target': 1, 'Non-Target': 0}).values\n",
    "        \n",
    "        # Create multi-class labels (specific gestures)\n",
    "        if self.gesture_encoder is None:\n",
    "            self.gesture_encoder = LabelEncoder()\n",
    "            self.gesture_encoder.fit(features_df['gesture'])\n",
    "        \n",
    "        if self.binary_encoder is None:\n",
    "            self.binary_encoder = LabelEncoder()\n",
    "            self.binary_encoder.fit(['Non-Target', 'Target'])\n",
    "        \n",
    "        multi_labels = self.gesture_encoder.transform(features_df['gesture'])\n",
    "        \n",
    "        # Select features\n",
    "        if imu_only:\n",
    "            # Use only IMU features (accelerometer and rotation)\n",
    "            feature_cols = [col for col in features_df.columns \n",
    "                           if col.startswith('acc_') or col.startswith('rot_')]\n",
    "        else:\n",
    "            # Use all sensor features\n",
    "            feature_cols = [col for col in features_df.columns \n",
    "                           if any(col.startswith(prefix) for prefix in \n",
    "                                 ['acc_', 'rot_', 'thm_', 'tof_'])]\n",
    "        \n",
    "        # Extract features\n",
    "        X = features_df[feature_cols].values\n",
    "        \n",
    "        return X, binary_labels, multi_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Only execute if PyTorch is available\n",
    "if TORCH_AVAILABLE:\n",
    "    class SimplifiedModel(nn.Module):\n",
    "        \"\"\"Simplified MLP model for statistical features.\"\"\"\n",
    "        \n",
    "        def __init__(self, input_dim, hidden_dims=[256, 128, 64], num_gestures=10, dropout=0.3):\n",
    "            super(SimplifiedModel, self).__init__()\n",
    "            \n",
    "            # Input layer\n",
    "            layers = [nn.Linear(input_dim, hidden_dims[0]), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            \n",
    "            # Hidden layers\n",
    "            for i in range(len(hidden_dims) - 1):\n",
    "                layers.extend([\n",
    "                    nn.Linear(hidden_dims[i], hidden_dims[i+1]),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout)\n",
    "                ])\n",
    "            \n",
    "            # Common feature extractor\n",
    "            self.feature_extractor = nn.Sequential(*layers)\n",
    "            \n",
    "            # Binary classification head (Target vs. Non-Target)\n",
    "            self.binary_head = nn.Linear(hidden_dims[-1], 2)\n",
    "            \n",
    "            # Multi-class classification head (specific gestures)\n",
    "            self.multi_head = nn.Linear(hidden_dims[-1], num_gestures)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Extract features\n",
    "            features = self.feature_extractor(x)\n",
    "            \n",
    "            # Binary classification\n",
    "            binary_logits = self.binary_head(features)\n",
    "            \n",
    "            # Multi-class classification\n",
    "            multi_logits = self.multi_head(features)\n",
    "            \n",
    "            return binary_logits, multi_logits\n",
    "    \n",
    "    class SensorFeatureDataset(Dataset):\n",
    "        \"\"\"Dataset for statistical features extracted from sensor data.\"\"\"\n",
    "        \n",
    "        def __init__(self, features, binary_labels, multi_labels=None):\n",
    "            self.features = torch.tensor(features, dtype=torch.float32)\n",
    "            self.binary_labels = torch.tensor(binary_labels, dtype=torch.long)\n",
    "            \n",
    "            if multi_labels is not None:\n",
    "                self.multi_labels = torch.tensor(multi_labels, dtype=torch.long)\n",
    "            else:\n",
    "                self.multi_labels = None\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.features)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            if self.multi_labels is not None:\n",
    "                return self.features[idx], self.binary_labels[idx], self.multi_labels[idx]\n",
    "            else:\n",
    "                return self.features[idx], self.binary_labels[idx], -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Competition Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def competition_score(binary_preds, binary_true, multi_preds, multi_true):\n",
    "    \"\"\"\n",
    "    Calculate the competition metric according to Kaggle's requirements:\n",
    "    1. Binary F1: Macro F1 score for target vs non-target classification\n",
    "    2. Gesture F1: Macro F1 score for gesture classification, where only target sequences are considered\n",
    "    Final score is the average of these two components.\n",
    "    \"\"\"\n",
    "    # 1. Binary F1 (target vs non-target)\n",
    "    binary_f1 = f1_score(binary_true, binary_preds, average='macro')\n",
    "    \n",
    "    # 2. Multi-class F1 (gesture classification for target sequences only)\n",
    "    # Only evaluate on target sequences (where binary_true == 1)\n",
    "    is_target = binary_true == 1\n",
    "    \n",
    "    # If there are target sequences in this batch, calculate F1\n",
    "    if np.sum(is_target) > 0:\n",
    "        multi_f1 = f1_score(\n",
    "            multi_true[is_target],  # True gesture labels for target sequences\n",
    "            multi_preds[is_target],  # Predicted gesture labels for target sequences\n",
    "            average='macro'\n",
    "        )\n",
    "    else:\n",
    "        # No target sequences in this batch\n",
    "        multi_f1 = 0.0\n",
    "    \n",
    "    # Final score is the average of the two components\n",
    "    final_score = (binary_f1 + multi_f1) / 2\n",
    "    \n",
    "    return final_score, binary_f1, multi_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Kaggle-Compliant Inference Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SequencePredictor:\n",
    "    \"\"\"\n",
    "    Memory-efficient predictor for BFRB sequences that follows the Kaggle API requirements.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir, zip_path, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the predictor.\n",
    "        \n",
    "        Args:\n",
    "            model_dir: Directory containing trained models\n",
    "            zip_path: Path to competition zip file\n",
    "            device: Device to run inference on ('cpu' or 'cuda')\n",
    "        \"\"\"\n",
    "        self.model_dir = model_dir\n",
    "        self.zip_path = zip_path\n",
    "        self.device = device\n",
    "        \n",
    "        # Load the processor\n",
    "        processor_path = os.path.join(model_dir, 'processor.joblib')\n",
    "        if os.path.exists(processor_path):\n",
    "            self.processor = joblib.load(processor_path)\n",
    "        else:\n",
    "            # Create a new processor if one doesn't exist\n",
    "            self.processor = SensorDataProcessor(zip_path, cache_dir=model_dir)\n",
    "        \n",
    "        # Load encoders\n",
    "        self.binary_encoder = joblib.load(os.path.join(model_dir, 'binary_encoder.joblib'))\n",
    "        self.gesture_encoder = joblib.load(os.path.join(model_dir, 'gesture_encoder.joblib'))\n",
    "        \n",
    "        # Load scaler\n",
    "        self.scaler = joblib.load(os.path.join(model_dir, 'scaler.joblib'))\n",
    "        \n",
    "        # Load models\n",
    "        self.models = []\n",
    "        \n",
    "        if TORCH_AVAILABLE:\n",
    "            for file in os.listdir(model_dir):\n",
    "                if file.startswith('model_fold') and file.endswith('.pt'):\n",
    "                    # Create model\n",
    "                    input_dim = len(self.scaler.mean_)\n",
    "                    num_gestures = len(self.gesture_encoder.classes_)\n",
    "                    \n",
    "                    model = SimplifiedModel(\n",
    "                        input_dim=input_dim,\n",
    "                        num_gestures=num_gestures\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    # Load weights\n",
    "                    model.load_state_dict(torch.load(\n",
    "                        os.path.join(model_dir, file),\n",
    "                        map_location=device\n",
    "                    ))\n",
    "                    \n",
    "                    # Set to eval mode\n",
    "                    model.eval()\n",
    "                    \n",
    "                    self.models.append(model)\n",
    "            \n",
    "            print(f\"Loaded {len(self.models)} models from {model_dir}\")\n",
    "    \n",
    "    def predict_sequence(self, sequence_id=None, sequence_df=None, file_path='test.csv'):\n",
    "        \"\"\"\n",
    "        Make a prediction for a single sequence, following Kaggle's API requirements.\n",
    "        The method can be called either with a sequence_id (which will be extracted from the data)\n",
    "        or directly with a sequence_df (for the API use case).\n",
    "        \n",
    "        Args:\n",
    "            sequence_id: ID of the sequence to predict\n",
    "            sequence_df: DataFrame containing the sequence data (alternative to sequence_id)\n",
    "            file_path: Path within the zip file to the test data\n",
    "            \n",
    "        Returns:\n",
    "            predicted_gesture: The predicted gesture\n",
    "        \"\"\"\n",
    "        # Extract the sequence if only sequence_id is provided\n",
    "        if sequence_df is None and sequence_id is not None:\n",
    "            sequence_df = self.processor.extract_sequence(file_path, sequence_id)\n",
    "        elif sequence_df is None and sequence_id is None:\n",
    "            raise ValueError(\"Either sequence_id or sequence_df must be provided\")\n",
    "        \n",
    "        # Preprocess the sequence\n",
    "        sequence_data = self.processor.preprocess_sequence(sequence_df)\n",
    "        \n",
    "        # Extract statistical features\n",
    "        features = self.processor.extract_statistical_features(sequence_data)\n",
    "        \n",
    "        # Create a feature vector\n",
    "        feature_vector = pd.DataFrame([features])\n",
    "        \n",
    "        # Drop non-feature columns\n",
    "        feature_vector = feature_vector.drop(columns=['sequence_id', 'subject'], errors='ignore')\n",
    "        \n",
    "        # Scale the features\n",
    "        X = self.scaler.transform(feature_vector)\n",
    "        \n",
    "        if TORCH_AVAILABLE and self.models:\n",
    "            # Convert to tensor\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            # Initialize arrays for predictions\n",
    "            binary_preds = np.zeros(2)\n",
    "            multi_preds = np.zeros(len(self.gesture_encoder.classes_))\n",
    "            \n",
    "            # Get predictions from all models\n",
    "            with torch.no_grad():\n",
    "                for model in self.models:\n",
    "                    binary_logits, multi_logits = model(X_tensor)\n",
    "                    \n",
    "                    binary_probs = torch.softmax(binary_logits, dim=1).cpu().numpy()\n",
    "                    multi_probs = torch.softmax(multi_logits, dim=1).cpu().numpy()\n",
    "                    \n",
    "                    binary_preds += binary_probs[0]\n",
    "                    multi_preds += multi_probs[0]\n",
    "            \n",
    "            # Average predictions\n",
    "            binary_preds /= len(self.models)\n",
    "            multi_preds /= len(self.models)\n",
    "            \n",
    "            # Get the predicted class\n",
    "            is_target = np.argmax(binary_preds) == 1  # Check if it's a target sequence\n",
    "            \n",
    "            if is_target:\n",
    "                # If it's a target, predict the specific gesture\n",
    "                gesture_id = np.argmax(multi_preds)\n",
    "                predicted_gesture = self.gesture_encoder.classes_[gesture_id]\n",
    "                \n",
    "                # Make sure the gesture is in the training set\n",
    "                if predicted_gesture not in self.gesture_encoder.classes_:\n",
    "                    print(f\"Warning: Predicted gesture '{predicted_gesture}' not found in training set. Defaulting to most confident known gesture.\")\n",
    "                    # Find the highest confidence for a known gesture\n",
    "                    known_classes_indices = [i for i, c in enumerate(self.gesture_encoder.classes_)]\n",
    "                    gesture_id = np.argmax(multi_preds[known_classes_indices])\n",
    "                    predicted_gesture = self.gesture_encoder.classes_[gesture_id]\n",
    "            else:\n",
    "                # If it's not a target, return \"non_target\" as required by Kaggle\n",
    "                predicted_gesture = \"non_target\"\n",
    "        else:\n",
    "            # Fallback if models aren't available\n",
    "            print(\"Warning: Models not available. Defaulting to 'non_target'\")\n",
    "            predicted_gesture = \"non_target\"\n",
    "        \n",
    "        return predicted_gesture"
   ]
  },
  {
   "cell_type": "markdown",
